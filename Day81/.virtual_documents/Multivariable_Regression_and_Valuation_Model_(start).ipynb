








# %pip install --upgrade plotly





import pandas as pd
import numpy as np

import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
# TODO: Add missing import statements





pd.options.display.float_format = '{:,.2f}'.format





data = pd.read_csv('boston.csv', index_col=0)








data.head()


data.tail()


data.sample()


# What is the shape of data?
# How many rows and columns does it have?

print(f"The Shape of the data is {data.shape}")
print(f"The data has  {data.shape[0]} rows and {data.shape[1]} columns")


# What are the column names?
data.columns





# Are there any NaN values or duplicates?
data.isnull().sum()


data.duplicated().sum()














# How many students are there per teacher on average?
data['PTRATIO'].mean()


# What is the average price of a home in the dataset?
data['PRICE'].mean()


# What is the CHAS feature?
# What are the minimum and the maximum value of the CHAS and why?

data['CHAS'].unique()
# CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)


# What is the maximum and the minimum number of rooms per dwelling in the dataset?
print(f"minimum number of rooms per dwelling in the dataset is{data['RM'].min()}")
print(f"maximum number of rooms per dwelling in the dataset is{data['RM'].max()}")








sns.displot(data=data,x='PRICE',kde=True)





sns.displot(data=data,x='DIS',kde=True)





sns.displot(data=data,x='RM',kde=True)





sns.displot(data=data,x='RAD',kde=True)





data['CHAS'].unique()


data['CHAS']=data['CHAS'].map({0.:'No',1.:'Yes'})


fig = px.bar(data_frame=data,
       x='CHAS',
       color='CHAS',
     )
fig.update_layout(xaxis_title='Property located next to river',
                 yaxis_title='Number of Homes',
                 title='Next to Charles river')
fig.show()











sns.pairplot(data[['NOX','DIS','RM','PRICE','LSTAT']])

















sns.jointplot(data=data,x='DIS',y='NOX')











sns.jointplot(data=data,x='INDUS',y='NOX')








sns.jointplot(data=data,x='LSTAT',y='RM')








sns.jointplot(data=data,x='LSTAT',y='PRICE')








sns.jointplot(data=data,x='RM',y='PRICE')








from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score


data.columns


X = data.drop(columns=['PRICE'])#'RM','NOX','DIS','CHAS','LSTAT'


X['CHAS']=X['CHAS'].map({'No':0,'Yes':1})


y=data['PRICE'].to_frame()


X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=10)


X['CHAS'].value_counts()











lr=LinearRegression()
lr.fit(X_train,y_train)


lr.score(X_train,y_train)











X.columns


pd.DataFrame(lr.coef_)


 # yes RM coefiicient is 4.09 which is positive so as RM increases Price also increases 


# LSTAT has -0.69 coeffient so when LSTAT increases Price decreases








predicted_values = lr.predict(X_train)


residuals = y_train-predicted_values








plt.scatter(x=y_train,y=predicted_values,c='darkblue')
plt.plot(y_train,y_train)
plt.xlabel('Actual price')
plt.ylabel('Predicted price')
plt.title('Actual vs Predicted values')


plt.scatter(x=predicted_values,y=residuals,c='darkblue')
plt.xlabel('Residuals')
plt.ylabel('Predicted values')
plt.title('Residuals vs Predicted values')





residuals.mean()


residuals.skew()


sns.displot(residuals,kind='kde')





sns.displot(y,kind='kde')


data['PRICE'].skew()


sns.displot(np.log(y),kind='kde')


np.log(y).skew()


# after using the log the skewnesss decreased





plt.figure(dpi=150)
plt.scatter(data.PRICE, np.log(data.PRICE))

plt.title('Mapping the Original Price to a Log Price')
plt.ylabel('Log Price')
plt.xlabel('Actual $ Price in 000s')
plt.show()





y2=np.log(y)


X_train2, X_test2, y_train2, y_test2 = train_test_split(X,y2,test_size=0.2,random_state=10)


lr2=LinearRegression()
lr2.fit(X_train2,y_train2)





predicted_values2 = lr2.predict(X_train2)


r2_score(y_train2,predicted_values2) #R2score have increased from 66 to 69





X.columns


lr2.coef_








residuals2=y_train2-predicted_values2





plt.scatter(x=y_train2,y=predicted_values2,c='darkblue')
plt.plot(y_train2,y_train2)
plt.xlabel('Actual price')
plt.ylabel('Predicted price')
plt.title('Actual vs Predicted values')


plt.scatter(x=predicted_values2,y=residuals2,c='darkblue')
plt.xlabel('Residuals')
plt.ylabel('Predicted values')
plt.title('Residuals vs Predicted values')








residuals2.mean()


residuals2.skew()





predicted_values=lr.predict(X_test)


predicted_values2=lr2.predict(X_test2)


r2_score(y_test,predicted_values)


r2_score(y_test2,predicted_values2)








data['CHAS']=data['CHAS'].map({'Yes':1,'No':0})


# Starting Point: Average Values in the Dataset
features = data.drop(['PRICE'], axis=1)
average_vals = features.mean().values
property_stats = pd.DataFrame(data=average_vals.reshape(1, len(features.columns)), 
                              columns=features.columns)
property_stats





np.exp(lr2.predict(property_stats))


y.mean()


Characteristics:

:Number of Instances: 506 

:Number of Attributes: 13 numeric/categorical predictive. The Median Value (attribute 14) is the target.

:Attribute Information (in order):
    1. CRIM     per capita crime rate by town
    2. ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
    3. INDUS    proportion of non-retail business acres per town
    4. CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
    5. NOX      nitric oxides concentration (parts per 10 million)
    6. RM       average number of rooms per dwelling
    7. AGE      proportion of owner-occupied units built prior to 1940
    8. DIS      weighted distances to five Boston employment centres
    9. RAD      index of accessibility to radial highways
    10. TAX      full-value property-tax rate per $10,000
        11. PTRATIO  pupil-teacher ratio by town
        12. B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
        13. LSTAT    % lower status of the population
        14. PRICE     Median value of owner-occupied homes in $1000's





# Solution:
# Starting Point: Average Values in the Dataset
features = data.drop(['PRICE'], axis=1)
average_vals = features.mean().values
property_stats = pd.DataFrame(data=average_vals.reshape(1, len(features.columns)), 
                              columns=features.columns)
property_stats



# Define Property Characteristics
next_to_river = True #CHAS
nr_rooms = 8 #RM
students_per_classroom = 20 #PTRATIO 
distance_to_town = 5  #DIS
pollution = data.NOX.quantile(q=0.75) # high  #NOX
amount_of_poverty =  data.LSTAT.quantile(q=0.25) # low #LSTAT


new_property=pd.DataFrame()


new_property['CRIM']=property_stats['CRIM']
new_property['ZN']=property_stats['ZN']
new_property['INDUS']=property_stats['INDUS']
new_property['CHAS']=next_to_river
new_property['NOX']=pollution
new_property['RM']=nr_rooms
new_property['AGE']=property_stats['AGE']
new_property['DIS']=distance_to_town
new_property['RAD']=property_stats['RAD']
new_property['TAX']=property_stats['TAX']
new_property['PTRATIO']=students_per_classroom
new_property['B']=property_stats['B']
new_property['LSTAT']=amount_of_poverty


new_property


np.exp(lr2.predict(new_property))



